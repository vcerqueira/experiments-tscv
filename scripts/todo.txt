# todo implement cv approaches
- [x] time-wise holdout
- [x] series-wise holdout
- [x] series-wise repeated holdout
- [x] series-wise k-fold cv
- [x] series-wise repeated k-fold cv
- [x] series-wise bootstrapping
- [x] series-wise repeated bootstrapping
- [x] series-wise monte carlo cv

hypotheses:
- series-wise (vertical+horizontal) splits will increase independence between training and testing sets during CV
- ... thus better performance estimates
- series-wise cv encourages finding models that generalize across time and across the domain (within-domain adaptable)

# todo learning methods
- [x] auto models: configuration selected using the validation set
    - configuration pool for RS is also fixed
- [ ] pre-defined pool of models: easier to run, not as realistic

# todo variants
- [x] standard rolling origin
- [ ] embargo
- [x] aggregation per series instead of fold
- [ ] aggregation per fold: not doing this, but it may be interesting to test
- [x] testset is 3*h cross-validation, as it should be more robust/stable.
assuming CV should be an offline thing (not repeated too often during deployment), so a larger window should be better


# todo analysis variants
- [x] selection v estimation
- [ ] sensitivity to sample size (number of series AND number of observations)
- [x] are we estimating the workflow (incl. HPO) or the final model (retrained OR not)?
very important consideration because the way we're doing creates different models for the estimation test set

# todo analysis
- [x] per dataset: dataset sample size issue (even with complete benchmarks probably)
- [x] per time series: disregarding potential dependencies among models and not so realistic
(we're not selecting models per uid) but solves the dataset size issue
- [x] per model -- Cv methods as workflows

# Notes
- focused on neural networks trained for cross-learning

# todo improvements
- add more models: 10 is enough, no?
- add more datasets-- those in typical benchmarks...